# -*- coding: utf-8 -*-
"""Bert_model_finallll.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ejP0dwmHpJo1cRTamnDtd4Gzd_H7aRMj
"""

import torch

if torch.cuda.is_available():
    device = torch.device("cuda")
    print("✅ GPU is available:", torch.cuda.get_device_name(0))
else:
    device = torch.device("cpu")
    print("⚠️ GPU not available, using CPU.")

pip install pymongo

import json

# List to store valid JSON objects
fixed_objects = []
buffer = ""       # Temporary string to hold object content
brace_count = 0   # Counter to track open/close braces

# Read the file line by line
with open("/content/diseases_final_for_model_cleaned.json", "r", encoding="utf-8") as f:
    for line in f:
        for char in line:
            # Count opening and closing braces
            if char == "{":
                brace_count += 1
            if char == "}":
                brace_count -= 1

            buffer += char

            # If one full object is completed (brace_count back to 0)
            if brace_count == 0 and buffer.strip():
                try:
                    # Try to parse JSON
                    obj = json.loads(buffer)
                    fixed_objects.append(obj)
                except json.JSONDecodeError:
                    # Skip broken objects and print warning
                    print("⚠️ Skipped broken object:", buffer[:120])
                buffer = ""

# Save the fixed objects into a new JSON file
with open("/content/diseases_final_for_model_fixed.json", "w", encoding="utf-8") as f:
    json.dump(fixed_objects, f, ensure_ascii=False, indent=2)

print("✅ Fixed file saved as diseases_final_for_model_fixed.json")
print("Total records:", len(fixed_objects))

#to save data in the mongo cloud
from pymongo import MongoClient

#  Load the fixed JSON file
with open("/content/diseases_final_for_model_fixed.json", "r", encoding="utf-8") as f:
    data = json.load(f)

#  MongoDB connection string
username = "NHSBB"
password = "E6tSgPvGAZ2uc3Q"
connection_string = f"mongodb+srv://{username}:{password}@cluster0.nsbnytx.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0"

client = MongoClient(connection_string)

#  Choose the database and collection
db = client["NHS_DB"]   # database name
collection = db["conditions_for_model"]  # collection name

#  Insert the data (list of dicts)
if isinstance(data, list):
    collection.insert_many(data)  # insert multiple records
else:
    collection.insert_one(data)   # insert single record

print("Data successfully uploaded to MongoDB Cloud!")

# this cell is to read data from mongo
from pymongo import MongoClient
import pandas as pd

# MongoDB connection details
username = "NHSBB"
password = "E6tSgPvGAZ2uc3Q"
connection_string = f"mongodb+srv://{username}:{password}@cluster0.nsbnytx.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0"

# Connect to MongoDB
client = MongoClient(connection_string)
db = client["NHS_DB"]
collection = db["conditions_for_model"]

print(" Connected to MongoDB")

# Read data from MongoDB into a DataFrame
data = list(collection.find({}, {"_id": 0}))
df = pd.DataFrame(data)

# Split the full dataframe (keep texts & labels aligned!)
from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Extract texts and labels
train_texts = train_df["input_text"].tolist()
train_labels = train_df["output_text"].tolist()

test_texts = test_df["input_text"].tolist()
test_labels = test_df["output_text"].tolist()

print("Train size:", len(train_texts))
print("Test size:", len(test_texts))

# Encode labels
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
all_labels = train_labels + test_labels
le.fit(all_labels)

train_labels_encoded = le.transform(train_labels)
test_labels_encoded = le.transform(test_labels)

print("Number of classes:", len(le.classes_))

#  Tokenization (BERT-base)
from transformers import AutoTokenizer

# Load BERT-base tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Tokenize train and test texts with max_length=512 to avoid RuntimeError
train_encodings = tokenizer(
    train_texts,
    truncation=True,
    padding=True,
    max_length=512
)
test_encodings = tokenizer(
    test_texts,
    truncation=True,
    padding=True,
    max_length=512
)

# Check a small sample
print(" Train encoding sample keys:", list(train_encodings.keys()))
print(" Length of train encodings:", len(train_encodings['input_ids']))
print(" Length of test encodings:", len(test_encodings['input_ids']))

# Dataset class
import torch

class MyDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# Check the class works by creating a small dummy dataset
dummy_dataset = MyDataset(train_encodings, train_labels_encoded)
print("Length of dummy dataset:", len(dummy_dataset))
print("First item sample:", {k: v.shape if isinstance(v, torch.Tensor) else v for k, v in dummy_dataset[0].items()})

# Create datasets
train_dataset = MyDataset(train_encodings, train_labels_encoded)
test_dataset = MyDataset(test_encodings, test_labels_encoded)

print("Dataset sizes:")
print("  Train dataset:", len(train_dataset))
print("  Test dataset:", len(test_dataset))

from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback

# Load the pre-trained BERT base model and set the number of output labels
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=len(le.classes_)
)

# Define the training arguments
training_args = TrainingArguments(
    output_dir="./results",              # directory to save results and checkpoints
    eval_strategy="epoch",               # evaluate at the end of each epoch
    save_strategy="epoch",               # save model after each epoch

    per_device_train_batch_size=16,      # training batch size
    per_device_eval_batch_size=16,       # evaluation batch size
    gradient_accumulation_steps=2,       # accumulate gradients to simulate a larger batch size

    num_train_epochs=4,                  # number of epochs to train
    learning_rate=2e-5,                  # set learning rate to 2e-5

    logging_dir="./logs",                # directory for training logs
    logging_steps=50,                    # log metrics every 50 steps
    save_total_limit=2,                  # keep only the last 2 checkpoints
    fp16=True,                           # use mixed precision for faster and lighter training
    report_to="none",                    # disable external logging tools (e.g., W&B)
    metric_for_best_model='eval_loss',   # metric to monitor for early stopping
    load_best_model_at_end=True,         # load the best model when training ends
)

# Create the Trainer object to handle training and evaluation
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)], # stop if no improvement for 2 evals
)

# Import the time module to measure training duration
import time

# Record the start time
start_time = time.time()

# Start training using the Hugging Face Trainer
trainer.train()

# Record the end time
end_time = time.time()

# Print the total training duration
print(f" Training finished in {end_time - start_time:.2f} seconds")

# Import numpy and accuracy_score
import numpy as np
from sklearn.metrics import accuracy_score

# Use the trainer to predict on the test dataset
predictions = trainer.predict(test_dataset)

# Get predicted class indices by taking argmax over logits
y_pred = np.argmax(predictions.predictions, axis=1)

# Get the true labels from the predictions object
y_true = predictions.label_ids

# Calculate accuracy and convert to percentage
accuracy = accuracy_score(y_true, y_pred) * 100

# Print the accuracy
print(f" Accuracy after Label Encoding: {accuracy:.2f}%")

import numpy as np
from sklearn.metrics import accuracy_score
import torch

# 1. Get predictions from the model on the test set
model.eval()
all_test_preds = []

with torch.no_grad():
    for batch in torch.utils.data.DataLoader(test_dataset, batch_size=16):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)
        all_test_preds.extend(preds.cpu().numpy())

# Decode predictions and actual labels for test set
decoded_test_predictions = le.inverse_transform(all_test_preds)
decoded_test_actuals = le.inverse_transform(test_labels_encoded)

# 2. Get predictions from the model on the train set
all_train_preds = []

with torch.no_grad():
    for batch in torch.utils.data.DataLoader(train_dataset, batch_size=16):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)
        all_train_preds.extend(preds.cpu().numpy())

# Decode predictions and actual labels for train set
decoded_train_predictions = le.inverse_transform(all_train_preds)
decoded_train_actuals = le.inverse_transform(train_labels_encoded)

# 3. Calculate accuracies
test_accuracy = accuracy_score(decoded_test_actuals, decoded_test_predictions)
train_accuracy = accuracy_score(decoded_train_actuals, decoded_train_predictions)

print("Train Accuracy:", train_accuracy)
print("Test Accuracy:", test_accuracy)

# Loss Curve
import matplotlib.pyplot as plt

logs = trainer.state.log_history

train_loss = [x["loss"] for x in logs if "loss" in x]
eval_loss = [x["eval_loss"] for x in logs if "eval_loss" in x]
epochs = range(1, len(eval_loss) + 1)

plt.plot(epochs, train_loss[:len(eval_loss)], label="Training Loss")
plt.plot(epochs, eval_loss, label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.title("Training vs Validation Loss")
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Calculate main evaluation metrics for the classification model
accuracy = accuracy_score(y_true, y_pred)                       # Percentage of correct predictions
precision = precision_score(y_true, y_pred, average='weighted') # How many predicted positives are actually correct
recall = recall_score(y_true, y_pred, average='weighted')       # How many actual positives were correctly identified
f1 = f1_score(y_true, y_pred, average='weighted')               # Harmonic mean of precision and recall

# Print the results clearly and simply
print("Accuracy:", round(accuracy, 3))
print("Precision:", round(precision, 3))
print("Recall:", round(recall, 3))
print("F1-score:", round(f1, 3))

trainer.save_model("./saved_model")
tokenizer.save_pretrained("./saved_model")

!zip -r saved_model.zip saved_model
