# -*- coding: utf-8 -*-
"""Another copy of Final processing.

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/167IeNC63pYPw9j__WjgV4cnFvFqH9kyi
"""

import json
import pandas as pd
import re
import copy

import json

# Step 1: Load the JSON file
with open("diseases_final.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# Step 2: Unified keyword map (used for both titles and content)
keyword_map = {
    "symptoms": [
        "symptom", "sign", "what is", "stages", "types",
        "pain", "swelling", "stiffness", "bruising", "discomfort", "ache",
        "characteristics", "common signs", "talking about", "what happened"
    ],
    "causes": [
        "cause", "risk factor", "what causes",
        "fracture", "injury", "smoking", "diabetes", "medication",
        "developed", "reaction", "drink alcohol"
    ],
    "diagnosis": [
        "diagnos", "test", "screen", "the procedure",
        "scan", "x-ray", "bone", "assessment", "examination",
        "testing", "tests", "challenge"
    ],
    "warnings": [
        "emergency", "warning", "seek medical advice", "complications",
        "when to get medical help", "fall", "risk", "not improved",
        "driving", "hospital", "avoid", "dial 999"
    ],
    "recommendations": [
        "treat", "recommend", "manage", "self-care", "prevention",
        "work", "recovery", "exercise", "rehabilitation", "massage",
        "raise", "balance", "diet", "active", "stop smoking", "return",
        "healthcare professional", "how", "surgery for", "what to do",
        "who is affected", "injections", "support", "is treated",
        "lifestyle", "occupational therapy", "how to do tasks at home",
        "treatments"
    ]
}

# Step 3: Function to map NHS section titles & content
def map_sections(sections: dict, condition_name: str):
    mapped = {k: [] for k in keyword_map.keys()}

    for title, content in sections.items():
        title_lower = title.lower()
        content_str = " ".join(content) if isinstance(content, list) else str(content)

        # --- 3.1: Match keywords in TITLE ---
        for category, keywords in keyword_map.items():
            if any(kw in title_lower for kw in keywords):
                mapped[category].append(content_str)

        # --- 3.2: Match keywords in CONTENT ---
        sentences = content_str.split(".")
        for sent in sentences:
            sent_lower = sent.lower().strip()

            # --- Special Rule: condition name or "about + condition" ---
            if condition_name.lower() in sent_lower or f"about {condition_name.lower()}" in sent_lower:
                mapped["symptoms"].append(sent.strip())
                continue

            for category, keywords in keyword_map.items():
                if any(kw in sent_lower for kw in keywords):
                    mapped[category].append(sent.strip())

    # Join lists into single strings
    for cat in mapped:
        mapped[cat] = " ".join([s for s in mapped[cat] if s])

    return mapped

# Step 4: Transform the dataset
structured = []
for item in data:
    condition_name = item.get("name", "")
    sections = map_sections(item.get("sections", {}), condition_name)
    structured.append({
        "condition": condition_name,
        "symptoms": sections["symptoms"],
        "causes": sections["causes"],
        "diagnosis": sections["diagnosis"],
        "warnings": sections["warnings"],
        "recommendations": sections["recommendations"]
    })

# Step 5: Save
with open("diseases_structured_sections_keywords.json", "w", encoding="utf-8") as f:
    json.dump(structured, f, ensure_ascii=False, indent=4)

print("✅ Data transformed: keywords checked in both titles & content (with condition rules).")

import json

# --- Step 1: Load transformed data ---
with open("diseases_structured_sections_keywords.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# --- Step 2: Function to check which sections are empty ---
def get_empty_sections(item):
    empty_sections = []
    for key in ["symptoms", "causes", "diagnosis", "warnings", "recommendations"]:
        if not item.get(key) or item[key].strip() == "":
            empty_sections.append(key)
    return empty_sections

# --- Step 3: List for conditions with ALL sections empty ---
no_sections = []

# --- Step 4: Iterate through the dataset ---
for item in data:
    condition = item.get("condition", "Unknown")
    empty_sections = get_empty_sections(item)

    # Case: All 5 sections are empty
    if len(empty_sections) == 5:
        no_sections.append(condition)

# --- Step 5: Print result ---
print("✅ Conditions with ALL sections empty:")
print(f"Count: {len(no_sections)}")
print(no_sections)

import json

# --- Step 1: Load transformed data ---
with open("diseases_structured_sections_keywords.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# --- Step 2: Function to check which sections are empty ---
def is_all_empty(item):
    for key in ["symptoms", "causes", "diagnosis", "warnings", "recommendations"]:
        if item.get(key) and item[key].strip() != "":
            return False
    return True

# --- Step 3: Drop conditions with ALL sections empty ---
filtered_data = [item for item in data if not is_all_empty(item)]

# --- Step 4: Save cleaned dataset ---
with open("diseases_cleaned.json", "w", encoding="utf-8") as f:
    json.dump(filtered_data, f, ensure_ascii=False, indent=4)

print(f"✅ Dropped conditions with ALL sections empty.")
print(f"Original count: {len(data)} → Cleaned count: {len(filtered_data)}")

# --- Step 1: Load cleaned data ---
with open("diseases_cleaned.json", "r", encoding="utf-8") as f:
    data = json.load(f)

 #--- Step 2: Preprocessing function (Unicode-safe lowercase) ---
def preprocess_text(text: str) -> str:
    if not text:
        return ""
    # Unicode-safe lowercase
    text = text.casefold()
    # Remove extra spaces
    text = re.sub(r"\s+", " ", text).strip()
    return text

# --- Step 3: Apply preprocessing to all sections ---
for item in data:
    for key in ["symptoms", "causes", "diagnosis", "warnings", "recommendations"]:
        item[key] = preprocess_text(item.get(key, ""))

# --- Step 4: Save preprocessed dataset ---
with open("diseases_preprocessed1.json", "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=4)

print("✅ Preprocessing done (Unicode cleaned + lowercased).")

#diseases_preprocessed1.json is data for transformer model

"""##____________________________________________________________________________

#Preprocessing
"""

# --- Load the existing preprocessed data ---
with open("diseases_preprocessed1.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# --- Make a deep copy ---
data_copy = copy.deepcopy(data)

# --- Save it as a new file ---
with open("diseases_preprocessed2.json", "w", encoding="utf-8") as f:
    json.dump(data_copy, f, ensure_ascii=False, indent=4)

print("✅ Copy created: diseases_preprocessed2.json")

pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz

# -----------------------------------------------
# Step 0: Load the installed SciSpacy small model
# -----------------------------------------------
# This is a lightweight model for scientific/medical texts
import spacy
nlp = spacy.load("en_core_sci_sm")

# -----------------------------------------------
# Step 1: Load your preprocessed2 data
# -----------------------------------------------
with open("diseases_preprocessed2.json", "r", encoding="utf-8") as f:
    df = json.load(f)

# -----------------------------------------------
# Step 2: Define the advanced medical processing function
# -----------------------------------------------
def advanced_medical_processing(text: str) -> str:
    """
    Perform advanced medical text processing using SciSpacy.

    Steps:
    1. Pass the text through the SciSpacy NLP pipeline.
    2. Extract medical entities detected by the model.
    3. Extract lemmatized tokens, ignoring stopwords and non-alphabetic tokens.
    4. Combine entities and tokens into a single cleaned text string.

    Args:
        text (str): Input medical text.

    Returns:
        str: Processed text ready for NLP/ML models.
    """
    if not text:
        return ""

    # Step 1: Apply SciSpacy NLP pipeline
    doc = nlp(text)

    # Step 2: Extract medical entities (e.g., diseases, drugs, procedures)
    entities = [ent.text for ent in doc.ents]

    # Step 3: Extract lemmatized tokens
    # - Skip stopwords (common words like "the", "and")
    # - Keep only alphabetic tokens (remove numbers, symbols)
    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]

    # Step 4: Merge entities and tokens into one string
    processed_text = " ".join(entities + tokens)

    return processed_text

# -----------------------------------------------
# Step 3: Apply the advanced processing to all relevant fields
# -----------------------------------------------
for item in data:
    for key in ["symptoms", "causes", "diagnosis", "warnings", "recommendations"]:
        item[key] = advanced_medical_processing(item.get(key, ""))

# -----------------------------------------------
# Step 4: Save the advanced processed dataset
# -----------------------------------------------
with open("diseases_advanced_processed.json", "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=4)

print("✅ Advanced SciSpacy processing done. Dataset ready for ML/Transformer models.")

import json

with open("diseases_advanced_processed.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# Example: print first disease
print(json.dumps(data[0], ensure_ascii=False, indent=4))

# -----------------------------------------------
# Step 5: Create input_text and output_text
# -----------------------------------------------
for item in data:
    # Input = symptoms + causes (even if one is empty)
    symptoms = item.get("symptoms", "")
    causes = item.get("causes", "")
    item["input_text"] = " ".join([symptoms, causes]).strip()

    # Output = disease + recommendations + warnings + diagnosis
    disease_name = item.get("disease_name", "")
    recommendations = item.get("recommendations", "")
    warnings = item.get("warnings", "")
    diagnosis = item.get("diagnosis", "")

    item["output_text"] = f"Disease: {disease_name} | Recommendations: {recommendations} | Warnings: {warnings} | Diagnosis: {diagnosis}"

# -----------------------------------------------
# Step 6: Save final dataset
# -----------------------------------------------
with open("diseases_final_for_model.json", "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=4)

print("✅ Advanced processing done. Dataset ready for ML/Transformer models.")