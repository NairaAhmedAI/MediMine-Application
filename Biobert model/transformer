# -*- coding: utf-8 -*-
"""BioBERT_Model_finalll.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yiZz8pBmbbnDYZpVwjsvw2SoNCXyOEGr
"""

import torch

if torch.cuda.is_available():
    device = torch.device("cuda")
    print("‚úÖ GPU is available:", torch.cuda.get_device_name(0))
else:
    device = torch.device("cpu")
    print("‚ö†Ô∏è GPU not available, using CPU.")

pip install pymongo

import json

# List to store valid JSON objects
fixed_objects = []
buffer = ""       # Temporary string to hold object content
brace_count = 0   # Counter to track open/close braces

# Read the file line by line
with open("/content/diseases_final_for_model_cleaned.json", "r", encoding="utf-8") as f:
    for line in f:
        for char in line:
            # Count opening and closing braces
            if char == "{":
                brace_count += 1
            if char == "}":
                brace_count -= 1

            buffer += char

            # If one full object is completed (brace_count back to 0)
            if brace_count == 0 and buffer.strip():
                try:
                    # Try to parse JSON
                    obj = json.loads(buffer)
                    fixed_objects.append(obj)
                except json.JSONDecodeError:
                    # Skip broken objects and print warning
                    print("‚ö†Ô∏è Skipped broken object:", buffer[:120])
                buffer = ""

# Save the fixed objects into a new JSON file
with open("/content/diseases_final_for_model_fixed.json", "w", encoding="utf-8") as f:
    json.dump(fixed_objects, f, ensure_ascii=False, indent=2)

print("‚úÖ Fixed file saved as diseases_final_for_model_fixed.json")
print("Total records:", len(fixed_objects))

#to save data in the mongo cloud
from pymongo import MongoClient

#  Load the fixed JSON file
with open("/content/diseases_final_for_model_fixed.json", "r", encoding="utf-8") as f:
    data = json.load(f)

#  MongoDB connection string
username = "NHSBB"
password = "E6tSgPvGAZ2uc3Q"
connection_string = f"mongodb+srv://{username}:{password}@cluster0.nsbnytx.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0"

client = MongoClient(connection_string)

#  Choose the database and collection
db = client["NHS_DB"]   # database name
collection = db["conditions_for_model"]  # collection name

#  Insert the data (list of dicts)
if isinstance(data, list):
    collection.insert_many(data)  # insert multiple records
else:
    collection.insert_one(data)   # insert single record

print("Data successfully uploaded to MongoDB Cloud!")

# this cell is to read data from mongo
from pymongo import MongoClient
import pandas as pd

# MongoDB connection details
username = "NHSBB"
password = "E6tSgPvGAZ2uc3Q"
connection_string = f"mongodb+srv://{username}:{password}@cluster0.nsbnytx.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0"

# Connect to MongoDB
client = MongoClient(connection_string)
db = client["NHS_DB"]
collection = db["conditions_for_model"]

print(" Connected to MongoDB")

# Read data from MongoDB into a DataFrame
data = list(collection.find({}, {"_id": 0}))
df = pd.DataFrame(data)

# Split the full dataframe (keep texts & labels aligned!)
# ‚úÖ Clean data before splitting
df = df.dropna(subset=['input_text', 'output_text'])
df = df.drop_duplicates(subset=['input_text', 'output_text'])
print('‚úÖ Data after cleaning:', len(df))

from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Extract texts and labels
train_texts = train_df["input_text"].tolist()
train_labels = train_df["output_text"].tolist()

test_texts = test_df["input_text"].tolist()
test_labels = test_df["output_text"].tolist()

print("Train size:", len(train_texts))
print("Test size:", len(test_texts))

# Encode labels
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
all_labels = train_labels + test_labels
le.fit(all_labels)

train_labels_encoded = le.transform(train_labels)
test_labels_encoded = le.transform(test_labels)

print("Number of classes:", len(le.classes_))

#  Step 1: Tokenization (BioBERT)
from transformers import AutoTokenizer

# Load BioBERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("dmis-lab/biobert-base-cased-v1.1")

# Tokenize train and test texts with max_length=512 to avoid RuntimeError
train_encodings = tokenizer(
    train_texts,
    truncation=True,
    padding=True,
    max_length=512
)
test_encodings = tokenizer(
    test_texts,
    truncation=True,
    padding=True,
    max_length=512
)

# Check a small sample
print(" Train encoding sample keys:", list(train_encodings.keys()))
print(" Length of train encodings:", len(train_encodings['input_ids']))
print(" Length of test encodings:", len(test_encodings['input_ids']))

# Step 2: Compute class weights to handle class imbalance
from sklearn.utils.class_weight import compute_class_weight
import torch
import numpy as np

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_labels),
    y=train_labels
)

class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)

print(" Class weights calculated successfully:")
print(class_weights_tensor)

# Dataset class
import torch

class MyDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# Check the class works by creating a small dummy dataset
dummy_dataset = MyDataset(train_encodings, train_labels_encoded)
print("Length of dummy dataset:", len(dummy_dataset))
print("First item sample:", {k: v.shape if isinstance(v, torch.Tensor) else v for k, v in dummy_dataset[0].items()})

# Create datasets
train_dataset = MyDataset(train_encodings, train_labels_encoded)
test_dataset = MyDataset(test_encodings, test_labels_encoded)

print("Dataset sizes:")
print("  Train dataset:", len(train_dataset))
print("  Test dataset:", len(test_dataset))

# üß© Step 3 & 4: Load BioBERT model + Weighted Trainer
from transformers import AutoModelForSequenceClassification, TrainingArguments, EarlyStoppingCallback, Trainer
import torch.nn as nn

model = AutoModelForSequenceClassification.from_pretrained(
    "dmis-lab/biobert-base-cased-v1.1",
    num_labels=len(le.classes_)
)

print(" BioBERT model loaded successfully!")

# üß© Step 4: Define Weighted Trainer
class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=0): # Added num_items_in_batch
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        loss_fct = nn.CrossEntropyLoss(weight=class_weights_tensor.to(model.device))
        loss = loss_fct(logits, labels)
        return (loss, outputs) if return_outputs else loss

print(" WeightedTrainer defined successfully!")

# üß© Step 5: Define Training Arguments

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",

    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=2,

    num_train_epochs=13,
    learning_rate=1e-5,

    logging_dir="./logs",
    logging_strategy="steps",
    logging_steps=10,

    save_total_limit=2,
    fp16=True,
    report_to="none",

    metric_for_best_model="eval_loss",
    load_best_model_at_end=True,
)

print(" Training arguments ready!")

# üß© Step 6: Train the model using WeightedTrainer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np
import time

def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    acc = accuracy_score(labels, preds)
    prec = precision_score(labels, preds, average='weighted', zero_division=0)
    rec = recall_score(labels, preds, average='weighted', zero_division=0)
    f1 = f1_score(labels, preds, average='weighted', zero_division=0)
    return {"accuracy": acc, "precision": prec, "recall": rec, "f1": f1}

trainer = WeightedTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)


start_time = time.time()
trainer.train()
end_time = time.time()

print(f" Training finished in {end_time - start_time:.2f} seconds")

import torch
from sklearn.metrics import accuracy_score

def get_predictions(model, dataset, device, le, labels_encoded, batch_size=16):
    model.eval()
    preds = []
    with torch.no_grad():
        for batch in torch.utils.data.DataLoader(dataset, batch_size=batch_size):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())

    #  Decode the predictions and the actual labels
    decoded_preds = le.inverse_transform(preds)
    decoded_actuals = le.inverse_transform(labels_encoded)

    return decoded_preds, decoded_actuals


# Get decoded predictions + accuracy for both train and test sets
train_preds, train_actuals = get_predictions(model, train_dataset, device, le, train_labels_encoded)
test_preds, test_actuals   = get_predictions(model, test_dataset, device, le, test_labels_encoded)

# Calculate accuracy
print("Train Accuracy:", accuracy_score(train_actuals, train_preds))
print("Test Accuracy:",  accuracy_score(test_actuals, test_preds))

# Loss Curve
import matplotlib.pyplot as plt

logs = trainer.state.log_history

train_loss = [x["loss"] for x in logs if "loss" in x]
eval_loss = [x["eval_loss"] for x in logs if "eval_loss" in x]
epochs = range(1, len(eval_loss) + 1)

plt.plot(epochs, train_loss[:len(eval_loss)], label="Training Loss")
plt.plot(epochs, eval_loss, label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.title("Training vs Validation Loss")
plt.show()

import joblib
joblib.dump(le, "label_encoder.joblib")

import shutil

model.save_pretrained("saved_model")
tokenizer.save_pretrained("saved_model")

shutil.make_archive("saved_model", 'zip', "saved_model")

print("‚úÖModel and tokenizer saved successfully and zipped as saved_model.zip!")

from sklearn.preprocessing import LabelEncoder
import joblib

# Recreate the encoder using the correct column
le = LabelEncoder()
le.fit(df["condition"])

# Save it as a separate file
joblib.dump(le, "label_encoder_correct.joblib")

print("‚úÖ Correct LabelEncoder saved as label_encoder_correct.joblib")
